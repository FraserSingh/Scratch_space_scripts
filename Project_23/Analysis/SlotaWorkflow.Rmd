---
title: "Slota workflow"
output: html_document
date: "2023-05-23"
---

## Seurat for Slota

Adapted from [here](https://bioconductor.org/packages/release/bioc/vignettes/DropletUtils/inst/doc/DropletUtils.html#reading-in-10x-genomics-data) and [here](https://www.singlecellcourse.org/scrna-seq-analysis-with-bioconductor.html).

```{r include=FALSE}
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("DropletUtils")
library(DropletUtils)

if (!require("irlba", quietly = TRUE))
    install.packages("irlba")

BiocManager::install("Seurat")
library(Seurat)
library(tidyverse)

#library(ggplot2)

library(dplyr)
library(stringr)
library(knitr)

remotes::install_github('chris-mcginnis-ucsf/DoubletFinder')
library(DoubletFinder)

if (!require("SoupX", quietly = TRUE))
    install.packages("SoupX")
library(SoupX)
library(Matrix)
```

```{r}
#Current working code 
#try using this next! as the counts, then add metadata.
Slota_counts<-read10xCounts("/localdisk/home/s2268606/University_Directories/Project_23/External_datasets/Slota_paper_original_format/", col.names=TRUE)

raw_objects_metadata<- read.csv("/localdisk/home/s2268606/University_Directories/Project_23/External_datasets/Slota_paper/metadata.csv")

cluster_file<-read.csv("/localdisk/home/s2268606/University_Directories/Project_23/External_datasets/Slota_paper_original_format/cluster_file.csv")

rawtable<-Read10X("/localdisk/home/s2268606/University_Directories/Project_23/External_datasets/Slota_paper/raw_feature_bc_matrix")

filteredtable<-Read10X("/localdisk/home/s2268606/University_Directories/Project_23/External_datasets/Slota_paper/filtered_feature_bc_matrix/")


filteredObject<-CreateSeuratObject(counts=filteredtable, meta.data = raw_objects_metadata)

rawObject<-CreateSeuratObject(counts=rawtable, meta.data = raw_objects_metadata )

#make into soupchannel but make more manual by setting FALSE parameter
sc = SoupChannel(rawtable, filteredtable, calcSoupProfile = FALSE)
sc = estimateSoup(sc)


#Calculate soup profile (if not done manually, causes issues downstream due to Nan values)
toc = sc$toc
scNoDrops = SoupChannel(toc, toc, calcSoupProfile = FALSE)
soupProf = data.frame(row.names = rownames(toc), est = rowSums(toc)/sum(toc), counts = rowSums(toc))
scNoDrops = setSoupProfile(scNoDrops, soupProf)


#Setup and apply clusters
cluster_file<-read.csv("/localdisk/home/s2268606/University_Directories/Project_23/External_datasets/Slota_paper_original_format/cluster_file.csv")

#Get the seurat clustering info available in the metadata
clusters<-raw_objects_metadata[2:nrow(raw_objects_metadata),c("NAME",'seurat_clusters')]

rownames(clusters)<-clusters$NAME
clusters$NAME<-NULL
clusters<-t(clusters)

#set clustering info
sc<-setClusters(scNoDrops,clusters)


sc<- autoEstCont(sc, verbose = T)
#issu here si that 2110 genes passed tf-idf cut-off and 287 soup quantile filter.  Taking the top 100.
# Warning: NaNs producedError in if (sum(clustExp) == 0) { : 
#   missing value where TRUE/FALSE needed


```

[investigating this source code to find](https://rdrr.io/github/constantAmateur/SoupX/src/R/setProperties.R#:~:text=if(length(clusters)!%3Dnrow(sc%24metaData))%7B%0A%20%20%20%20%20%20stop(%22Invalid%20cluster%20specification.%20%20See%20help.%22)%0A%20%20%20%20%7Delse%7B) sources of error\
\

```{r}
folder_path_slota<-c("/localdisk/home/s2268606/University_Directories/Project_23/External_datasets/Slota_paper")

#load Slota paper ('original gene expression matrices generated by cellranger' according to https://singlecell.broadinstitute.org/single_cell/study/SCP1962/dysregulation-of-neuroprotective-astrocytes-a-spectrum-of-microglial-activation-states-and-altered-hippocampal-neurogenesis-are-revealed-by-single-cell-rna-sequencing-in-prion-disease#study-download)

Slota_counts<-read10xCounts("/localdisk/home/s2268606/University_Directories/Project_23/External_datasets/Slota_paper_original_format/", col.names=TRUE)

#or

#load the matrix.mtx, metadata.csv and barcode.tsv manually
cluster_file<-read.csv("/localdisk/home/s2268606/University_Directories/Project_23/External_datasets/Slota_paper_original_format/cluster_file.csv")

raw_objects_metadata<- read.csv("/localdisk/home/s2268606/University_Directories/Project_23/External_datasets/Slota_paper/metadata.csv")

rawtable<-Read10X("/localdisk/home/s2268606/University_Directories/Project_23/External_datasets/Slota_paper/raw_feature_bc_matrix")

filteredtable<-Read10X("/localdisk/home/s2268606/University_Directories/Project_23/External_datasets/Slota_paper/filtered_feature_bc_matrix/")

rawObject<-CreateSeuratObject(counts=rawtable, meta.data = raw_objects_metadata )

filteredObject<-CreateSeuratObject(counts=filteredtable, meta.data = raw_objects_metadata)
#or

#Use soupx to load the stuff
Slota_soupx<-load10X("/localdisk/home/s2268606/University_Directories/Project_23/External_datasets/Slota_paper_original_format/")

```

```{r}
par(mar = c(1, 1, 1, 1))

current_data <- autoEstCont(Slota_counts)

#save soup-less matrix
souped_matrix <-adjustCounts(current_data, roundToInt = T)


#Hopefully use the reduced matrix
current_obj <- CreateSeuratObject(counts = souped_matrix, project = "Ximerakis")

# Make names for and create the Seurat objects
objName <- c("Slota_countsobj")

assign(objName, current_obj)
```

```{r}
#get log2 counts
assay(Slota_sce, "logcounts")<-log2(counts(Slota_sce)+1)
```

```{r}
logcounts(Slota_sce)[1:10, 1:4]
```

```{r}
#get mean counts per cell
colData(Slota_sce)$mean_counts <- colMeans(counts(Slota_sce))
colData(Slota_sce)
```

```{r}
#Sum of counts in each cell
colData(Slota_sce)$total_counts <- colSums(counts(Slota_sce))
```

```{r}
#Counts per million
# function sweep takes following arguments:
# matrix to normalize
# dimention to normalize along (1 - by rows, 2 - by columns)
# statistics to normalize by
# function to use for normalization
assay(Slota_sce, "cpm") <- sweep(counts(Slota_sce),2,Slota_sce$total_counts/1e6,'/')
# check that column sums are 1e6 now
colSums(cpm(Slota_sce))[1:10]
```

## Filtering (might need to be repeated after some EDA)

```{r}
#we can retain genes with mean count > 0.01
gene_means <- rowMeans(counts(Slota_sce))
Slota_sce[gene_means > 0.01, ]

#We can filter by a threshold of genes
#how many genes per cell
total_detected_per_cell <- colSums(counts(Slota_sce) > 0)

#Use vector to apply final condition (at least 1k genes as suggested by Nick on 23/05/2023)
Slota_sce_filtered <- Slota_sce[, total_detected_per_cell >= 1000]


#Filter by read number. At least 3.5k reads(counts) is too harsh so commented out (see note below)
#cell_filter <- colSums(counts(Slota_sce)) >= 3500
# check how many TRUE/FALSE have
#table(cell_filter)
#Slota_sce_filtered <- Slota_sce[ , cell_filter]

Slota_sce_filtered
```

The Slota_sce_filtered object has only 38 cells left after all of the filtering above was applied, so filters will need to be relaxed. Without the 3.5 k reads filter there were 90318 cells left.

```{r}
#trying to annotate with ensembl, but think that the matrix file might already have this.
#check that the rownames , keytypes
#gene_names <- mapIds(org.Mm.eg.db, keys=rownames(counts(Slota_sce_filtered)), keytype='ENSEMBL',columns='SYMBOL', column='SYMBOL')

#The following gave 0 mitochondrial genes, has the dataset been filtered already?

ensdb_genes<- genes(EnsDb.Mmusculus.v79)
MT_names<- ensdb_genes[seqnames(ensdb_genes)=="MT"]$gene_id
is_mito<-rownames(counts(Slota_sce_filtered) %in% MT_names
)
is_mito<-rownames(counts(Slota_sce_filtered)) %in% MT_names
table(is_mito)
```

```{r}
#could do without filtering
#cell_info <- as.data.frame(colData(Slota_sce))

cell_info_filtered <- as.data.frame(colData(Slota_sce))
cell_info_filtered$rownames <- metadata$biosample_id[match(cell_info_filtered$Barcode, metadata$NAME)]
```

```{r}
library(ggplot2)

ggplot(data = cell_info_filtered, aes(x = rownames, y = total_counts)) +
  geom_violin(fill = 'brown') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(title = "Total counts per cell", x = "Sample Origin (by Brain Region)")

```
